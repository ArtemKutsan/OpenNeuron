{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from openneuron.units import Neuron\n",
    "from openneuron.activations import *\n",
    "from openneuron.losses import *\n",
    "from openneuron.optimizers import *\n",
    "from openneuron.initializers import *\n",
    "from openneuron.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # Класс перенесен в units.py\\n# Класс для нейрона\\nclass Neuron:\\n    count = 0\\n\\n    def __init__(self, weights=HeUniform(), bias=HeUniform(), input_size=2, output_size=1, activation=linear, loss=MSE(), optimizer=SGD(momentum=0)):\\n        Neuron.count += 1\\n        self.number = Neuron.count  # номер нейрона\\n        self.id = id(self)\\n        self.layer = None  # ссылка на слой в котором находится нейрон\\n        self.index = None  # индекс нейрона в слое\\n        self.X = None\\n        self.weights = np.array([weights]).T if isinstance(weights, list) else weights(shape=(input_size, output_size))\\n        self.bias = np.array([[bias]]).T if isinstance(bias, (float, int)) else bias(shape=(1, output_size))\\n        self.activation = activation or linear  # функция активации нейрона\\n        self.loss_function = loss or MSE()\\n        self.optimizer = optimizer or SGD(momentum=0)\\n        self.gradient = {\\'weights\\': None, \\'bias\\': None}\\n\\n    @property\\n    def Inputs(self):\\n        if self.layer is not None:\\n            return self.layer.Inputs\\n        return self.X\\n    \\n    @Inputs.setter\\n    def Inputs(self, X):\\n        self.X = X\\n\\n    @property\\n    def inputs(self):\\n        return self.Inputs[-1] if self.Inputs is not None else None\\n\\n    @property\\n    def Z(self):\\n        if self.layer is not None:\\n            return self.layer.Z.T[self.index]\\n        return np.dot(self.Inputs, self.weights) + (self.bias.item() if self.bias.item() is not None else 0) if self.Inputs is not None else None\\n\\n    @property\\n    def z(self):\\n        return self.Z[-1].item() if self.Z is not None else None\\n    \\n    @property\\n    def A(self):\\n        if self.layer is not None:\\n            return self.layer.A.T[self.index]\\n        return self.activation(self.Z) if self.Z is not None else None\\n\\n    @property\\n    def a(self):\\n        return self.A[-1].item() if self.A is not None else None\\n    \\n    def activation_derivative(self, Z):\\n        return self.activation(Z, derivative=True)\\n    \\n    def forward(self, Inputs, training=True):\\n        self.Inputs = Inputs\\n        return self.A\\n\\n    def backward(self, error):\\n        self.delta = self.loss_function.loss_derivative(error) * self.activation_derivative(self.Z)\\n        self.gradient[\\'weights\\'] = np.dot(self.Inputs.T, self.delta)\\n        self.gradient[\\'bias\\'] = np.sum(self.delta, axis=0)\\n        # return np.dot(self.delta, self.weights.T)  # delta ошибки передаваемая в следующий слой\\n\\n    def update(self):\\n        self.optimizer.update(self)\\n\\n    def fit(self, X, y, epochs=10, batch_size=1, final_batch_size=None, shuffle=True, validation_data=None):\\n        self.X = X\\n        self.epochs = epochs\\n        self.batch_size = batch_size\\n        self.final_batch_size = final_batch_size\\n        \\n        if validation_data is not None:\\n            X_test, y_test = validation_data\\n            val_data_type = \\'Test Data\\'\\n        else:\\n            X_test, y_test = X, y\\n            val_data_type = \\'Train Data\\'\\n        \\n        val_loss = self.loss_function.evaluate_loss(y_test, self.predict(X_test))\\n        print(f\\'Training started with Overall Loss {val_loss:.4f} on {val_data_type}\\')\\n        \\n        self.X_len = X.shape[0]\\n        for epoch in range(epochs):\\n            if self.batch_size is not None and self.final_batch_size is not None:\\n                batch_size = self.batch_size + int((self.final_batch_size - self.batch_size) * (epoch + 1) / epochs)\\n            \\n            if shuffle:\\n                permutation = np.random.permutation(self.X_len)\\n                X_shuffled = X[permutation]\\n                y_shuffled = y[permutation]\\n            else:\\n                X_shuffled = X\\n                y_shuffled = y\\n            \\n            epoch_loss = 0\\n            num_batches = 0\\n            \\n            for i in range(0, self.X_len, (batch_size or self.X_len)):\\n                num_batches += 1\\n                X_batch = X_shuffled[i:i+(batch_size or self.X_len)]\\n                y_batch = y_shuffled[i:i+(batch_size or self.X_len)]\\n                \\n                predictions_batch = self.forward(X_batch)\\n                error = self.loss_function.evaluate_error(y_batch, predictions_batch)\\n                loss = self.loss_function.evaluate_loss(y_batch, predictions_batch)\\n                epoch_loss += loss\\n                \\n                self.backward(error)\\n                self.update()\\n            \\n            loss = epoch_loss / num_batches\\n            val_loss = self.loss_function.evaluate_loss(y_test, self.predict(X_test))\\n            \\n            # Выводим на новой строке только каждую 10-ю эпоху\\n            end = \\'\\n\\' if (epochs < 10 or epoch == 0 or (epoch + 1) % (epochs // 10) == 0 or epoch == epochs - 1) else \\'\\r\\' \\n            print(f\\'Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}, Validation Loss: {val_loss:.4f} on {val_data_type}\\', end=end)\\n        print(f\\'Training completed with Overall Loss {val_loss:.4f} on {val_data_type}\\')\\n\\n    def predict(self, X):\\n        return self.forward(X, training=False)\\n    \\n    def call(self, X, training=False):\\n        \"\"\" # Метод call вызывается после выполнения встроенного метода __call__.\\n        Служит для дополнения/изменения работы встроенного метода __call__ не нарушая необходимый для правильной работы процесс\\n        вычисления данных при вызове объекта. На вход получает входные данные (массив X). По умолчанию возвращает значение(я) \\n        активированного выхода нейрона (массив A).\\n        \"\"\"\\n        # Любой дополнительный код\\n        return self.A\\n\\n    def __call__(self, Inputs, training=False):\\n        self.forward(Inputs, training)\\n        return self.call(Inputs, training)\\n\\n    def __str__(self):\\n        return f\\'Neuron {format(self.number, decimals=decimals)}, inputs: {format(self.inputs, decimals=decimals)}, \\'                f\\'weights: {format(self.weights.flatten(), decimals=decimals)}, bias: {format(self.bias.item(), decimals=decimals)}, \\'                f\\'z: {format(self.z, decimals=decimals)}, activation: {self.activation.__name__}, a: {format(self.a, decimals=decimals)}\\'\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' # Класс перенесен в units.py\n",
    "# Класс для нейрона\n",
    "class Neuron:\n",
    "    count = 0\n",
    "\n",
    "    def __init__(self, weights=HeUniform(), bias=HeUniform(), input_size=2, output_size=1, activation=linear, loss=MSE(), optimizer=SGD(momentum=0)):\n",
    "        Neuron.count += 1\n",
    "        self.number = Neuron.count  # номер нейрона\n",
    "        self.id = id(self)\n",
    "        self.layer = None  # ссылка на слой в котором находится нейрон\n",
    "        self.index = None  # индекс нейрона в слое\n",
    "        self.X = None\n",
    "        self.weights = np.array([weights]).T if isinstance(weights, list) else weights(shape=(input_size, output_size))\n",
    "        self.bias = np.array([[bias]]).T if isinstance(bias, (float, int)) else bias(shape=(1, output_size))\n",
    "        self.activation = activation or linear  # функция активации нейрона\n",
    "        self.loss_function = loss or MSE()\n",
    "        self.optimizer = optimizer or SGD(momentum=0)\n",
    "        self.gradient = {'weights': None, 'bias': None}\n",
    "\n",
    "    @property\n",
    "    def Inputs(self):\n",
    "        if self.layer is not None:\n",
    "            return self.layer.Inputs\n",
    "        return self.X\n",
    "    \n",
    "    @Inputs.setter\n",
    "    def Inputs(self, X):\n",
    "        self.X = X\n",
    "\n",
    "    @property\n",
    "    def inputs(self):\n",
    "        return self.Inputs[-1] if self.Inputs is not None else None\n",
    "\n",
    "    @property\n",
    "    def Z(self):\n",
    "        if self.layer is not None:\n",
    "            return self.layer.Z.T[self.index]\n",
    "        return np.dot(self.Inputs, self.weights) + (self.bias.item() if self.bias.item() is not None else 0) if self.Inputs is not None else None\n",
    "\n",
    "    @property\n",
    "    def z(self):\n",
    "        return self.Z[-1].item() if self.Z is not None else None\n",
    "    \n",
    "    @property\n",
    "    def A(self):\n",
    "        if self.layer is not None:\n",
    "            return self.layer.A.T[self.index]\n",
    "        return self.activation(self.Z) if self.Z is not None else None\n",
    "\n",
    "    @property\n",
    "    def a(self):\n",
    "        return self.A[-1].item() if self.A is not None else None\n",
    "    \n",
    "    def activation_derivative(self, Z):\n",
    "        return self.activation(Z, derivative=True)\n",
    "    \n",
    "    def forward(self, Inputs, training=True):\n",
    "        self.Inputs = Inputs\n",
    "        return self.A\n",
    "\n",
    "    def backward(self, error):\n",
    "        self.delta = self.loss_function.loss_derivative(error) * self.activation_derivative(self.Z)\n",
    "        self.gradient['weights'] = np.dot(self.Inputs.T, self.delta)\n",
    "        self.gradient['bias'] = np.sum(self.delta, axis=0)\n",
    "        # return np.dot(self.delta, self.weights.T)  # delta ошибки передаваемая в следующий слой\n",
    "\n",
    "    def update(self):\n",
    "        self.optimizer.update(self)\n",
    "\n",
    "    def fit(self, X, y, epochs=10, batch_size=1, final_batch_size=None, shuffle=True, validation_data=None):\n",
    "        self.X = X\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.final_batch_size = final_batch_size\n",
    "        \n",
    "        if validation_data is not None:\n",
    "            X_test, y_test = validation_data\n",
    "            val_data_type = 'Test Data'\n",
    "        else:\n",
    "            X_test, y_test = X, y\n",
    "            val_data_type = 'Train Data'\n",
    "        \n",
    "        val_loss = self.loss_function.evaluate_loss(y_test, self.predict(X_test))\n",
    "        print(f'Training started with Overall Loss {val_loss:.4f} on {val_data_type}')\n",
    "        \n",
    "        self.X_len = X.shape[0]\n",
    "        for epoch in range(epochs):\n",
    "            if self.batch_size is not None and self.final_batch_size is not None:\n",
    "                batch_size = self.batch_size + int((self.final_batch_size - self.batch_size) * (epoch + 1) / epochs)\n",
    "            \n",
    "            if shuffle:\n",
    "                permutation = np.random.permutation(self.X_len)\n",
    "                X_shuffled = X[permutation]\n",
    "                y_shuffled = y[permutation]\n",
    "            else:\n",
    "                X_shuffled = X\n",
    "                y_shuffled = y\n",
    "            \n",
    "            epoch_loss = 0\n",
    "            num_batches = 0\n",
    "            \n",
    "            for i in range(0, self.X_len, (batch_size or self.X_len)):\n",
    "                num_batches += 1\n",
    "                X_batch = X_shuffled[i:i+(batch_size or self.X_len)]\n",
    "                y_batch = y_shuffled[i:i+(batch_size or self.X_len)]\n",
    "                \n",
    "                predictions_batch = self.forward(X_batch)\n",
    "                error = self.loss_function.evaluate_error(y_batch, predictions_batch)\n",
    "                loss = self.loss_function.evaluate_loss(y_batch, predictions_batch)\n",
    "                epoch_loss += loss\n",
    "                \n",
    "                self.backward(error)\n",
    "                self.update()\n",
    "            \n",
    "            loss = epoch_loss / num_batches\n",
    "            val_loss = self.loss_function.evaluate_loss(y_test, self.predict(X_test))\n",
    "            \n",
    "            # Выводим на новой строке только каждую 10-ю эпоху\n",
    "            end = '\\n' if (epochs < 10 or epoch == 0 or (epoch + 1) % (epochs // 10) == 0 or epoch == epochs - 1) else '\\r' \n",
    "            print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}, Validation Loss: {val_loss:.4f} on {val_data_type}', end=end)\n",
    "        print(f'Training completed with Overall Loss {val_loss:.4f} on {val_data_type}')\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.forward(X, training=False)\n",
    "    \n",
    "    def call(self, X, training=False):\n",
    "        \"\"\" # Метод call вызывается после выполнения встроенного метода __call__.\n",
    "        Служит для дополнения/изменения работы встроенного метода __call__ не нарушая необходимый для правильной работы процесс\n",
    "        вычисления данных при вызове объекта. На вход получает входные данные (массив X). По умолчанию возвращает значение(я) \n",
    "        активированного выхода нейрона (массив A).\n",
    "        \"\"\"\n",
    "        # Любой дополнительный код\n",
    "        return self.A\n",
    "\n",
    "    def __call__(self, Inputs, training=False):\n",
    "        self.forward(Inputs, training)\n",
    "        return self.call(Inputs, training)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'Neuron {format(self.number, decimals=decimals)}, inputs: {format(self.inputs, decimals=decimals)}, ' \\\n",
    "               f'weights: {format(self.weights.flatten(), decimals=decimals)}, bias: {format(self.bias.item(), decimals=decimals)}, ' \\\n",
    "               f'z: {format(self.z, decimals=decimals)}, activation: {self.activation.__name__}, a: {format(self.a, decimals=decimals)}'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started with Overall Loss 0.4335 on Train Data\n",
      "Epoch 1/100, Loss: 0.4319, Validation Loss: 0.3012 on Train Data\n",
      "Epoch 10/100, Loss: 0.1408, Validation Loss: 0.1109 on Train Data\n",
      "Epoch 20/100, Loss: 0.0828, Validation Loss: 0.0677 on Train Data\n",
      "Epoch 30/100, Loss: 0.0566, Validation Loss: 0.0481 on Train Data\n",
      "Epoch 40/100, Loss: 0.0422, Validation Loss: 0.0369 on Train Data\n",
      "Epoch 50/100, Loss: 0.0333, Validation Loss: 0.0297 on Train Data\n",
      "Epoch 60/100, Loss: 0.0273, Validation Loss: 0.0247 on Train Data\n",
      "Epoch 70/100, Loss: 0.0230, Validation Loss: 0.0211 on Train Data\n",
      "Epoch 80/100, Loss: 0.0198, Validation Loss: 0.0183 on Train Data\n",
      "Epoch 90/100, Loss: 0.0174, Validation Loss: 0.0162 on Train Data\n",
      "Epoch 100/100, Loss: 0.0154, Validation Loss: 0.0145 on Train Data\n",
      "Training completed with Overall Loss 0.0145 on Train Data\n",
      "\n",
      "Predictions:\n",
      "[[0.0041]\n",
      " [0.1311]\n",
      " [0.1338]\n",
      " [0.8492]]\n",
      "\n",
      "Last forward:\n",
      "Neuron, inputs: [1 1], weights: [3.62 3.6], bias: -5.49, z: 1.73, activation: sigmoid, a: 0.85\n"
     ]
    }
   ],
   "source": [
    "# Пример AND\n",
    "# Настройка вывода для удобочитаемости\n",
    "np.set_printoptions(precision=4, suppress=True, threshold=6, edgeitems=2, linewidth=80)\n",
    "\n",
    "X = np.array([[0, 0], \n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "y = np.array([[0],\n",
    "              [0],\n",
    "              [0],\n",
    "              [1]])\n",
    "\n",
    "X_train, y_train = X, y\n",
    "X_test, y_test = X, y\n",
    "\n",
    "neuron = Neuron(weights=[0.45, -0.12], bias=1.0, activation=sigmoid, loss=MSE(), optimizer=SGD(learning_rate=0.7, momentum=0.3))\n",
    "neuron.fit(X_train, y_train, epochs=100, shuffle=False)\n",
    "\n",
    "# y_pred = neuron.predict(X_test)\n",
    "y_pred = neuron(X_test)\n",
    "print(f'\\nPredictions:\\n{y_pred}')\n",
    "print(f'\\nLast forward:\\n{neuron}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started with Overall Loss 0.5697 on Train Data\n",
      "Epoch 1/10, Loss: 0.5697, Validation Loss: 0.3851 on Train Data\n",
      "Epoch 2/10, Loss: 0.3851, Validation Loss: 0.2552 on Train Data\n",
      "Epoch 3/10, Loss: 0.2552, Validation Loss: 0.1647 on Train Data\n",
      "Epoch 4/10, Loss: 0.1647, Validation Loss: 0.1044 on Train Data\n",
      "Epoch 5/10, Loss: 0.1044, Validation Loss: 0.0659 on Train Data\n",
      "Epoch 6/10, Loss: 0.0659, Validation Loss: 0.0419 on Train Data\n",
      "Epoch 7/10, Loss: 0.0419, Validation Loss: 0.0271 on Train Data\n",
      "Epoch 8/10, Loss: 0.0271, Validation Loss: 0.0179 on Train Data\n",
      "Epoch 9/10, Loss: 0.0179, Validation Loss: 0.0122 on Train Data\n",
      "Epoch 10/10, Loss: 0.0122, Validation Loss: 0.0085 on Train Data\n",
      "Training completed with Overall Loss 0.0085 on Train Data\n",
      "\n",
      "Predictions:\n",
      "[[0.9078]]\n",
      "\n",
      "Last forward:\n",
      "Neuron, inputs: [1 0], weights: [0.65 0.9], bias: 1.64, z: 2.29, activation: sigmoid, a: 0.91\n"
     ]
    }
   ],
   "source": [
    "# Пример простой регрессии\n",
    "# Настройка вывода для удобочитаемости\n",
    "np.set_printoptions(precision=4, suppress=True, threshold=6, edgeitems=2, linewidth=80)\n",
    "\n",
    "X = np.array([[1, 0]])\n",
    "y = np.array([[1]])\n",
    "\n",
    "X_train, y_train = X, y\n",
    "X_test, y_test = X, y\n",
    "\n",
    "neuron = Neuron(activation=sigmoid, loss=MSE(), optimizer=Adam(learning_rate=1))\n",
    "neuron.fit(X_train, y_train, epochs=10)\n",
    "\n",
    "# y_pred = neuron.predict(X_test)\n",
    "y_pred = neuron(X_test)\n",
    "print(f'\\nPredictions:\\n{y_pred}')\n",
    "print(f'\\nLast forward:\\n{neuron}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.164  -0.0596]\n",
      " [ 0.2807  0.1232]\n",
      " [-0.0974  0.2396]\n",
      " [ 0.7205  0.7953]], shape=(4, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "initializer = tf.keras.initializers.GlorotNormal()\n",
    "values = initializer(shape=(4, 2))\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started with Overall Loss 0.3786 on Train Data\n",
      "Epoch 1/25, Loss: 0.3193, Validation Loss: 0.2017 on Train Data\n",
      "Epoch 2/25, Loss: 0.1989, Validation Loss: 0.1149 on Train Data\n",
      "Epoch 4/25, Loss: 0.1226, Validation Loss: 0.1125 on Train Data\n",
      "Epoch 6/25, Loss: 0.1007, Validation Loss: 0.0932 on Train Data\n",
      "Epoch 8/25, Loss: 0.0877, Validation Loss: 0.0801 on Train Data\n",
      "Epoch 10/25, Loss: 0.0777, Validation Loss: 0.0730 on Train Data\n",
      "Epoch 12/25, Loss: 0.0682, Validation Loss: 0.0632 on Train Data\n",
      "Epoch 14/25, Loss: 0.0618, Validation Loss: 0.0569 on Train Data\n",
      "Epoch 16/25, Loss: 0.0552, Validation Loss: 0.0496 on Train Data\n",
      "Epoch 18/25, Loss: 0.0472, Validation Loss: 0.0440 on Train Data\n",
      "Epoch 20/25, Loss: 0.0440, Validation Loss: 0.0388 on Train Data\n",
      "Epoch 22/25, Loss: 0.0391, Validation Loss: 0.0347 on Train Data\n",
      "Epoch 24/25, Loss: 0.0340, Validation Loss: 0.0313 on Train Data\n",
      "Epoch 25/25, Loss: 0.0315, Validation Loss: 0.0290 on Train Data\n",
      "Training completed with Overall Loss 0.0290 on Train Data\n",
      "\n",
      "Predictions:\n",
      "[[0.0144]\n",
      " [0.1977]\n",
      " [0.189 ]\n",
      " [0.7974]]\n",
      "\n",
      "Last forward:\n",
      "Neuron, inputs: [1 1], weights: [2.77 2.83], bias: -4.23, z: 1.37, activation: sigmoid, a: 0.8\n"
     ]
    }
   ],
   "source": [
    "# Пример AND\n",
    "# Настройка вывода для удобочитаемости\n",
    "np.set_printoptions(precision=4, suppress=True, threshold=6, edgeitems=2, linewidth=80)\n",
    "\n",
    "X = np.array([[0, 0], \n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "y = np.array([[0],\n",
    "              [0],\n",
    "              [0],\n",
    "              [1]])\n",
    "\n",
    "X_train, y_train = X, y\n",
    "X_test, y_test = X, y\n",
    "\n",
    "neuron = Neuron(activation=sigmoid, optimizer=Adam(learning_rate=1))\n",
    "neuron.fit(X_train, y_train, epochs=25)\n",
    "\n",
    "# y_pred = neuron.predict(X_test)\n",
    "y_pred = neuron(X_test)\n",
    "print(f'\\nPredictions:\\n{y_pred}')\n",
    "print(f'\\nLast forward:\\n{neuron}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started with Overall Loss 0.7500 on Train Data\n",
      "Epoch 1/5, Loss: 0.5000, Validation Loss: 0.2500 on Train Data\n",
      "Epoch 2/5, Loss: 0.5000, Validation Loss: 0.0000 on Train Data\n",
      "Epoch 3/5, Loss: 0.0000, Validation Loss: 0.0000 on Train Data\n",
      "Epoch 4/5, Loss: 0.0000, Validation Loss: 0.0000 on Train Data\n",
      "Epoch 5/5, Loss: 0.0000, Validation Loss: 0.0000 on Train Data\n",
      "Training completed with Overall Loss 0.0000 on Train Data\n",
      "\n",
      "Predictions:\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "\n",
      "Last forward:\n",
      "Neuron, inputs: [1 1], weights: [1.49 0.28], bias: 0.38, z: 2.15, activation: heaviside/step, a: 1\n"
     ]
    }
   ],
   "source": [
    "# Пример OR\n",
    "# Настройка вывода для удобочитаемости\n",
    "np.set_printoptions(precision=4, suppress=True, threshold=6, edgeitems=2, linewidth=80)\n",
    "\n",
    "X = np.array([[0, 0], \n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "y = np.array([[0],\n",
    "              [1],\n",
    "              [1],\n",
    "              [1]])\n",
    "\n",
    "X_train, y_train = X, y\n",
    "X_test, y_test = X, y\n",
    "\n",
    "neuron = Neuron(activation=heaviside, loss=MAE(), optimizer=SGD(learning_rate=1, momentum=0))\n",
    "neuron.fit(X_train, y_train, epochs=5)\n",
    "\n",
    "# y_pred = neuron.predict(X_test)\n",
    "y_pred = neuron(X_test)\n",
    "print(f'\\nPredictions:\\n{y_pred}')\n",
    "print(f'\\nLast forward:\\n{neuron}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started with Overall Loss 0.2500 on Train Data\n",
      "Epoch 1/10, Loss: 0.2500, Validation Loss: 0.2500 on Train Data\n",
      "Epoch 2/10, Loss: 0.2500, Validation Loss: 0.2500 on Train Data\n",
      "Epoch 3/10, Loss: 0.5000, Validation Loss: 0.2500 on Train Data\n",
      "Epoch 4/10, Loss: 0.5000, Validation Loss: 0.0000 on Train Data\n",
      "Epoch 5/10, Loss: 0.0000, Validation Loss: 0.0000 on Train Data\n",
      "Epoch 6/10, Loss: 0.0000, Validation Loss: 0.0000 on Train Data\n",
      "Epoch 7/10, Loss: 0.0000, Validation Loss: 0.0000 on Train Data\n",
      "Epoch 8/10, Loss: 0.0000, Validation Loss: 0.0000 on Train Data\n",
      "Epoch 9/10, Loss: 0.0000, Validation Loss: 0.0000 on Train Data\n",
      "Epoch 10/10, Loss: 0.0000, Validation Loss: 0.0000 on Train Data\n",
      "Training completed with Overall Loss 0.0000 on Train Data\n",
      "\n",
      "Predictions:\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      "\n",
      "Last forward:\n",
      "Neuron, inputs: [1 1], weights: [1.05 1.02], bias: -1.18, z: 0.89, activation: heaviside/step, a: 1\n"
     ]
    }
   ],
   "source": [
    "# Пример AND\n",
    "# Настройка вывода для удобочитаемости\n",
    "np.set_printoptions(precision=4, suppress=True, threshold=6, edgeitems=2, linewidth=80)\n",
    "\n",
    "X = np.array([[0, 0], \n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "y = np.array([[0],\n",
    "              [0],\n",
    "              [0],\n",
    "              [1]])\n",
    "\n",
    "X_train, y_train = X, y\n",
    "X_test, y_test = X, y\n",
    "\n",
    "neuron = Neuron(activation=heaviside, loss=MAE(), optimizer=SGD(learning_rate=1, momentum=0))\n",
    "neuron.fit(X_train, y_train, epochs=10)\n",
    "\n",
    "# y_pred = neuron.predict(X_test)\n",
    "y_pred = neuron(X_test)\n",
    "print(f'\\nPredictions:\\n{y_pred}')\n",
    "print(f'\\nLast forward:\\n{neuron}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
