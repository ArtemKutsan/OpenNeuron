{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# С чего все началось"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Еще не Перцептрон"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Общий алгоритм\n",
    "\n",
    "Предположим, что у нас есть какие-то данные. Назовем их входными данными. Также мы знаем, какой у нас должен быть ответ. Назовем его выходными данными. Наша задача — написать программу, в которую будут подаваться наши входные данные, после чего наша программа будет производить над этими данными определенные вычисления (сложение/умножение входных данных на некоторые заданные значения) и выдавать ответ. Если этот ответ не соответствует тому, который должен быть, наша программа должна изменить некоторые из своих ранее заданных параметров так, чтобы ответы совпадали.\n",
    "\n",
    "Запутанно? На самом деле все гораздо проще, чем может показаться на первый взгляд. По сути, нам нужно найти решение некоторого уравнения путем изменения его некоторых переменных. Эти переменные можно назвать коэффициентами. Или, как мы их будем называть, веса и смещение.\n",
    "\n",
    "Тут можно сделать небольшое отступление в сторону школьной математики и увидеть, что далее в коде мы решаем уравнение вида \n",
    "f(x) = a*x + b, где \"f(x)\" — это функция от \"x\" и наше значение \"y\", \"a\" — это наш вес (weight), а \"b\" — это наше смещение (bias). И у данного уравнения существует вполне себе простое математическое решение по нахождению корней. Зачем тогда мы изобретаем велосипед? Но это лишь самый упрощенный пример, поняв который, можно в целом понять, как устроено обучение нейронных сетей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Входные данные (x): 1\n",
      "Ожидаемое/необходимое значение (y): 1\n",
      "Начальные значения (до обучения) веса (weight): -0.5 и смещения (bias): 0.25\n",
      "\n",
      "Тест до обучения\n",
      "Для входных данных 1 предсказанное значение: 0 (output: -0.25), ожидаемое/истинное значение: 1\n",
      "\n",
      "Процесс обучения из 5 эпох\n",
      "Эпоха обучения 1/5, weight: -0.40, bias: 0.35\n",
      "Эпоха обучения 2/5, weight: -0.30, bias: 0.45\n",
      "Эпоха обучения 3/5, weight: -0.20, bias: 0.55\n",
      "Эпоха обучения 4/5, weight: -0.10, bias: 0.65\n",
      "Эпоха обучения 5/5, weight: -0.10, bias: 0.65\n",
      "\n",
      "Тест после обучения\n",
      "Для входных данных 1 предсказанное значение: 1 (output: 0.55), ожидаемое/истинное значение: 1\n"
     ]
    }
   ],
   "source": [
    "# Общий алгоритм\n",
    "\n",
    "# Входные данные и целевая метка (ожидаемый результат)\n",
    "x = 1\n",
    "y = 1\n",
    "\n",
    "# Гиперпараметры (этими параметрами мы можем влиять на процесс обучения)\n",
    "epochs = 5  # количество итераций для изменения веса и смещения в процессе обучения\n",
    "h = 0.1  # размер шага изменения веса и смещения\n",
    "\n",
    "# Вес и смещение\n",
    "weight = -0.5  # начальный вес (этот параметр мы будем \"тренировать\"/изменять в процессе обучения)\n",
    "bias = 0.25  # начальное смещение (этот параметр мы будем \"тренировать\"/изменять в процессе обучения)\n",
    "\n",
    "# Выход нашей программы до обучения \n",
    "output = x * weight + bias  # мы взяли наши входные данные (x), умножили на вес (weight) и прибавили смещение (bias)\n",
    "# Функция активации: ступенчатая/пороговая функция (порог 0.5)\n",
    "if output >= 0.5:\n",
    "    prediction = 1  # все, что выше и равно 0.5, считается 1\n",
    "else:\n",
    "    prediction = 0  # все, что ниже 0.5, считается 0\n",
    "print(f'Входные данные (x): {x}')\n",
    "print(f'Ожидаемое/необходимое значение (y): {y}')\n",
    "print(f'Начальные значения (до обучения) веса (weight): {weight} и смещения (bias): {bias}')\n",
    "\n",
    "print('\\nТест до обучения')\n",
    "print(f'Для входных данных {x} предсказанное значение: {prediction} (output: {output:.2f}), ожидаемое/истинное значение: {y}')\n",
    "\n",
    "print(f'\\nПроцесс обучения из {epochs} эпох')\n",
    "# Обучение (процесс подбора/изменения весов и смещения для нахождения решения задачи)\n",
    "for epoch in range(epochs):\n",
    "    # Вычисление взвешенной (weight) суммы входного сигнала (x) и смещения (bias)\n",
    "    output = x * weight + bias  # это то, что выдает/предсказывает наш алгоритм\n",
    "    \n",
    "    # Функция активации: ступенчатая/пороговая функция (порог 0.5)\n",
    "    if output >= 0.5:\n",
    "        prediction = 1  # все, что выше и равно 0.5, считается 1\n",
    "    else:\n",
    "        prediction = 0  # все, что ниже 0.5, считается 0\n",
    "    \n",
    "    # Вычисление ошибки\n",
    "    error = y - prediction\n",
    "    \n",
    "    # Обновление/изменение веса и смещения на основе ошибки error и с использованием шага h\n",
    "    weight += h * error * x\n",
    "    bias += h * error\n",
    "    \n",
    "    # Отладочная информация\n",
    "    print(f'Эпоха обучения {epoch + 1}/{epochs}, weight: {weight:.2f}, bias: {bias:.2f}')\n",
    "\n",
    "# Предсказание (выход) на обучающем примере\n",
    "print('\\nТест после обучения')\n",
    "output = x * weight + bias\n",
    "if output >= 0.5:\n",
    "    prediction = 1\n",
    "else:\n",
    "    prediction = 0\n",
    "print(f'Для входных данных {x} предсказанное значение: {prediction} (output: {output:.2f}), ожидаемое/истинное значение: {y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Код выше демонстрирует общий алгоритм обучения нейрона. Алгоритм обучения нейронной сети несколько сложнее, но в целом похож. Конечно, в коде нет никакой сети и даже нет никаких нейронов, а также отсутствует одна из важнейших концепций нейронных сетей, такая как МОР (метод обратного распространения ошибки). Однако, в нем есть вычисление ошибки и корректировка параметров на основе этой ошибки. В целом, код демонстрирует общий алгоритм того, что происходит при обучении.\n",
    "\n",
    "Код написан специально максимально упрощенно, без использования массивов, матриц, сторонних библиотек, функций, ООП и других сложных концепций. Все это будет добавляться постепенно. В конце у нас будет полноценная реализация нейронной сети со слоями и нейронами на основе объектов и классов, с множеством различных функций и даже небольшим \"костылем\" для Keras, с помощью которого мы сможем сравнить результаты работы нашей простой нейронной сети с результатами, которые выдает нейронная сеть, построенная на Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, давайте разберем и улучшим наш код. На самом деле задача, которую мы решаем выше, довольно бесполезная и в целом практического смысла никакого не имеет. Повторюсь, она лишь демонстрирует алгоритм обучения нейрона. То есть она показывает, что алгоритм получает входные данные, производит с ними некие операции (сложение/умножение), сравнивает результат с тем результатом, который мы задали как правильный, и если ответ не совпадает, то алгоритм пытается таким образом изменить вес и смещение, чтобы ответ совпадал с верным.\n",
    "\n",
    "Изначально мы задали вес и смещение как: weight = -0.5, bias = 0.25. Наши входные данные были просто одним числом: x = 1, и мы сказали, что y (правильный ответ) должен быть равен 1. Наша функция вычисления y выглядит так: output = x * weight + bias. Output — это то, что мы получаем, и ожидаем, что он будет равен 1. Получается: output = 1 * (-0.5) + 0.25 = -0.25. Применяем к значению -0.25 функцию активации. В данном случае, если output >= 0.5, то мы говорим, что мы получили/предсказали для x = 1 ответ 0. Что неправильно, так как мы ожидаем ответ 1. Мы считаем ошибку: error = y - prediction или error = 1 - 0 = 1. То есть ошибка равна 1. Дальше мы корректируем вес и смещение, используя эту ошибку. weight += h * error * x, weight += 0.1 * 1 * 1, weight += 0.1, то есть weight = -0.5 + 0.1, weight = -0.40. Таким же образом изменяем смещение: bias += 0.1 * 1, bias += 0.1, то есть bias = 0.25 + 0.1, bias = 0.35.\n",
    "\n",
    "Первая эпоха закончилась. Теперь у нас weight = -0.40, а bias = 0.35. Какой же теперь output? output = 1 * (-0.40) + 0.35 = -0.05. Напомню, что при начальных весе и смещении output был -0.25. Мы видим, что мы стали ближе к 1, но наш прогноз после применения функции активации всё равно 0, так как функция активации даст прогноз 1, если наш output будет равен или более 0.5. Но мы указали 5 эпох для обучения. Начинаем вторую. Можете сами подставить вес и смещение в формулу output = x * weight + bias, посчитать результат и ошибку и убедиться в том, что наш вес и смещение станут: weight = -0.30, bias = 0.45. Теперь output = 1 * (-0.30) + 0.45 = 0.15, то есть еще ближе к нашему правильному ответу. Проделав 4 эпохи обучения, наши вес и смещение станут: weight = -0.10, bias = 0.65. И output станет: output = 1 * (-0.10) + 0.65 = 0.55. Теперь наше предсказание после активации для x = 1 станет 1 (0.55 > 0.5). Значит, наша ошибка теперь будет: error = 1 - 1 = 0. То есть вес и смещение не изменятся: weight += 0 * 0.1 * 1, weight += 0, weight = -0.10, как и был. То же самое произойдет для смещения. Как мы видим, на 5-й эпохе обучения вес и смещение не изменились. В качестве примера можно изменить целевое значение y и изменить, например, начальное значение bias и посмотреть, как будут меняться вес и смещение. Так же мв немного улучшим наш код оформив функцию активации как отдельную функцию в программе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Входные данные (x): 1\n",
      "Ожидаемое/необходимое значение (y): 0\n",
      "Начальные значения (до обучения) веса (weight): -0.5 и смещения (bias): 1.25\n",
      "\n",
      "Тест до обучения\n",
      "Для входных данных 1 предсказанное значение: 1 (output: 0.75), ожидаемое/истинное значение: 0\n",
      "\n",
      "Процесс обучения из 5 эпох\n",
      "Эпоха обучения 1/5, weight: -0.60, bias: 1.15\n",
      "Эпоха обучения 2/5, weight: -0.70, bias: 1.05\n",
      "Эпоха обучения 3/5, weight: -0.70, bias: 1.05\n",
      "Эпоха обучения 4/5, weight: -0.70, bias: 1.05\n",
      "Эпоха обучения 5/5, weight: -0.70, bias: 1.05\n",
      "\n",
      "Тест после обучения\n",
      "Для входных данных 1 предсказанное значение: 0 (output: 0.35), ожидаемое/истинное значение: 0\n"
     ]
    }
   ],
   "source": [
    "# Общий алгоритм\n",
    "\n",
    "# Входные данные и целевая метка (ожидаемый результат)\n",
    "x = 1\n",
    "y = 0\n",
    "\n",
    "# Гиперпараметры (этими параметрами мы можем влиять на процесс обучения)\n",
    "epochs = 5  # количество итераций для изменения веса и смещения в процессе обучения\n",
    "h = 0.1  # размер шага изменения веса и смещения\n",
    "\n",
    "# Вес и смещение\n",
    "weight = -0.5  # начальный вес (этот параметр мы будем \"тренировать\"/изменять в процессе обучения)\n",
    "bias = 1.25  # начальное смещение (этот параметр мы будем \"тренировать\"/изменять в процессе обучения)\n",
    "\n",
    "# Функция активации: ступенчатая/пороговая функция (порог 0.5)\n",
    "def activation(value):\n",
    "    return 1 if value >= 0.5 else 0\n",
    "\n",
    "# Выход нашей программы до обучения \n",
    "output = x * weight + bias  # мы взяли наши входные данные (x), умножили на вес (weight) и прибавили смещение (bias)\n",
    "predicton = activation(output)\n",
    "print(f'Входные данные (x): {x}')\n",
    "print(f'Ожидаемое/необходимое значение (y): {y}')\n",
    "print(f'Начальные значения (до обучения) веса (weight): {weight} и смещения (bias): {bias}')\n",
    "\n",
    "print('\\nТест до обучения')\n",
    "print(f'Для входных данных {x} предсказанное значение: {prediction} (output: {output:.2f}), ожидаемое/истинное значение: {y}')\n",
    "\n",
    "print(f'\\nПроцесс обучения из {epochs} эпох')\n",
    "# Обучение (процесс подбора/изменения весов и смещения для нахождения решения задачи)\n",
    "for epoch in range(epochs):\n",
    "    # Вычисление взвешенной (weight) суммы входного сигнала (x) и смещения (bias)\n",
    "    output = x * weight + bias  # это то, что выдает/предсказывает наш алгоритм\n",
    "    prediction = activation(output)\n",
    "    \n",
    "    # Вычисление ошибки\n",
    "    error = y - prediction\n",
    "    \n",
    "    # Обновление/изменение веса и смещения на основе ошибки error и с использованием шага h\n",
    "    weight += h * error * x\n",
    "    bias += h * error\n",
    "    \n",
    "    # Отладочная информация\n",
    "    print(f'Эпоха обучения {epoch + 1}/{epochs}, weight: {weight:.2f}, bias: {bias:.2f}')\n",
    "\n",
    "# Предсказание на обучающем примере\n",
    "print('\\nТест после обучения')\n",
    "output = x * weight + bias\n",
    "predicton = activation(output)\n",
    "print(f'Для входных данных {x} предсказанное значение: {prediction} (output: {output:.2f}), ожидаемое/истинное значение: {y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы видим, что изначально у нас было предсказание 1 а мы ожидали 0. Используя ошибку предсказания мы немного подкоррестировали вес и смещение и получили уже на второй эпоке нужный результат."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Общие понятия и терминология\n",
    "\n",
    "Давайте определимся с некоторыми понятиями и необходимой терминологией для дальнейшего продвижения и обобщим алгоритм обучения.\n",
    "\n",
    "1. **Входные данные (`X`)**:\n",
    "   - Набор данных, подаваемый на вход модели (сети/слоя/нейрона) для обучения или предсказания. Обычно представляется в виде матрицы, где строки соответствуют примерам, а столбцы — признакам. Мы пока будем пользоваться списками в котороых каждый элемент это значение некоторго параметра объекта или фича от английского features. В наборе X содержатся объекты x со своими признаками. \n",
    "   \n",
    "2. **Целевая метка (`y`)**:\n",
    "   - Значения, которые модель должна предсказать. В задаче классификации это могут быть классы, а в задаче регрессии — числовые значения.\n",
    "   \n",
    "3. **Вес(а) (`weights`)**:\n",
    "   - Параметры модели, которые обучаются в процессе тренировки. Влияние каждого признака на итоговое предсказание регулируется весами.\n",
    "   \n",
    "4. **Смещение (`bias`)**:\n",
    "   - Дополнительный параметр модели, который помогает лучше подгонять модель под данные. Он позволяет модели предсказывать ненулевые значения, даже если все входные признаки равны нулю.\n",
    "   \n",
    "5. **Значение `z` (значение функции или вычисленное значение взвешенной суммы входных данных и смещения)**:\n",
    "   - Промежуточное значение, вычисляемое как линейная комбинация входных данных и весов, с добавлением смещения.\n",
    "   - Формула: `z = weights * x + bias`. Если, например, входныеданные это один объект `x = [1, 0]` из выборки `X`, то у нейрона на вход которому будут поступать эти данные также должно быть два веса, например `weights = [0.25, -0.5]` и если у нейрона есть `bias = 0.75` например, то наше значение  функции будет вычисляться так: `z = w1*x1 + w2*x2 + bias`, то есть `z` на каждом этапе будет вычисляться по такой формуле (конечно это если у нас просто один нейрон, в случае сети формула `z` и кол-во весов следующего слоя будет зависеть от кол-ва нейронов в предыдущем слое).   \n",
    "\n",
    "6. **Функция активации (activation function или просто `activation`)**:\n",
    "   - Функция, применяемая к промежуточному значению `z`, чтобы ввести нелинейность в модель и помочь ей решать сложные задачи.\n",
    "   - Пример: `ReLU (Rectified Linear Unit)`, `sigmoid`, `tanh`. У перцептрона который мы создадим ступенчатая функция активации которая называется часто `heaviside`.\n",
    "\n",
    "7. **Выход (`output` или просто `a` в смысле значения активации)**:\n",
    "   - Конечное предсказание модели, которое получается после применения функции активации к промежуточному значению `z`. Тут кстати внимательный читатель заметит, что я ранее называл `output`-ом значение `z`. Я так делал для упрощения понимания работы алгоритмя на начатьных этапах. Далее я буду использовать `z` и `a` вместо `output`. \n",
    "\n",
    "8. **Ошибка (`loss`/`error`)**:\n",
    "   - Разница между предсказанным значением и реальной целевой меткой. Ошибка используется для оценки качества модели и дальнейшего её обучения. В задачах регрессии может использоваться MSE (Mean Squared Error), а в задачах классификации — Categorical Cross-Entropy. Тут так же можно заметить, что мы вычисляем ошибку как `y - prediction`, а в первом предложении говорится, что это разница `prediction - y`. На самом деле это не принципиально. Просто мы пользуемся `y - prediction` так как это интуитивно более понятно. Но мы можем так же пользоватьмся и `prediction - y` просто тогда мы должны менять веса не `weights +=` а `weights -=`. Пока мы будем пользоваться только понятием `error` для упрощения и считать ошибку как `y - prediction`. Возможно далее, в процессе реализации Categorical Cross-Entropy станет понятно почему там используется `prediction - y`.\n",
    "\n",
    "9. **Функция потерь (loss function)**:\n",
    "   - Функция, которая вычисляет ошибку. Она служит для того, чтобы алгоритм мог понять, насколько хорошо или плохо он работает и как ему нужно скорректировать свои параметры, например MSE или Cross-Entropy. Опять же пока не будем использовать это чтобы не усложнять.  \n",
    "\n",
    "10. **Предсказание (`prediction`)**:\n",
    "    - Значение, которое модель возвращает после обработки входных данных. В идеале оно должно быть близким к истинному значению. В задаче классификации, например, это может быть класс, к которому, по мнению модели, относится входной пример."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.3 Обобщенное описание процесса обучения и предсказания\n",
    "\n",
    "1. **Подготовка данных**: Определяем входные данные `X` или просто один пример `x` и целевые метки `y`.\n",
    "\n",
    "2. **Инициализация модели**: Устанавливаем начальные значения весов и смещений.\n",
    "\n",
    "3. **Метод/процесс прямого распространения (Forward Pass)**:\n",
    "   - \"Подаем\" наши данные в сеть/нейрон. \n",
    "   - Вычисляем промежуточное значение `z` для всех нейпонов/слов сети или просто для одного нейрона.\n",
    "   - Применяем функцию активации к `z`, получая `a` (`output`).\n",
    "\n",
    "4. **Вычисление ошибки**:\n",
    "   - Сравниваем значение активированного выхода `a` (`output`) с истинным значением `y`.\n",
    "   - Используем функцию потерь для расчета ошибки. Пока просто будем использовать значение `error`.\n",
    "\n",
    "5. **Метод/процесс обратного распространения (Backward Pass)**:\n",
    "   - Вычисляем градиенты функции потерь по отношению к весам и смещениям на выходном слое/нейроне. Вот тут мы пока не будем пользоваться понятием градиент и производная. Да и в целом пока мы не перейдем непосредственно к сети из слоев с нейронами мы не будем называть это методом обратного распространения ошибки (МОР), так как она (ошибка) пока еще не распространияется у нас, то есть не передается в предыдущие слои сети.\n",
    "   - Обновляем веса и смещения с использованием алгоритма оптимизации, например, градиентного спуска. Тут туже пока несколько упростим процесс и не будем применять понятие \"алгоритм оптимизации\" опять же для упрощения на начальных этапах.\n",
    "\n",
    "6. **Повторение**:\n",
    "   - Повторяем шаги 3-5 на протяжении заданного количества эпох или до достижения приемлемого уровня ошибки.\n",
    "\n",
    "7. **Предсказание**:\n",
    "   - После обучения модели, подаем новые данные на вход и получаем предсказание. Так как для начала мы не будем использовать большие выборки которые должны быть разделены на обучающую и тестовую, то проедсказывать мы будем то на чем обучались. Это неправильно но опять же зависит от задачи.\n",
    "\n",
    "В итоге хочется сказать, что некоторые неточности в терминологии допущены вынужденно/намерено для общего упрощения понимания задачи. В дальнейшем все эти неточности будут естесственным образом исправлены.\n",
    "\n",
    "Итак давайте перейдем от задачи которая в принципе имела мало смысла к чему-то более осмысленному. А именно давайте теперь рассмотрим некий один объект `x` нашей некой выборки X, у которого есть два признака. Те есть `x = [1, 0]`. Перепишем наш код с учетом той терминологии что мы описали выше, также обобщим формулу вычисления `z`, и добавим еще функций. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Входные данные (x): [1, 0]\n",
      "Ожидаемое/необходимое значение (y): 0\n",
      "Начальные значения (до обучения) весов (weights): [-0.5, -1.25] и смещения (bias): 1.75\n",
      "\n",
      "Тест до обучения\n",
      "Для входных данных [1, 0] предсказанное значение: 1, ожидаемое/истинное значение: 0\n",
      "\n",
      "Процесс обучения из 5 эпох\n",
      "Эпоха обучения 1/5, weights: [-0.6, -1.25], bias: 1.65\n",
      "Эпоха обучения 2/5, weights: [-0.7, -1.25], bias: 1.55\n",
      "Эпоха обучения 3/5, weights: [-0.8, -1.25], bias: 1.45\n",
      "Эпоха обучения 4/5, weights: [-0.9, -1.25], bias: 1.35\n",
      "Эпоха обучения 5/5, weights: [-0.9, -1.25], bias: 1.35\n",
      "\n",
      "Тест после обучения\n",
      "Для входных данных [1, 0] предсказанное значение: 0, ожидаемое/истинное значение: 0\n"
     ]
    }
   ],
   "source": [
    "# Общий алгоритм\n",
    "\n",
    "# Входные данные и целевая метка (ожидаемый результат)\n",
    "x = [1, 0]  # объект с двумя признаками\n",
    "y = 0\n",
    "\n",
    "# Гиперпараметры (этими параметрами мы можем влиять на процесс обучения)\n",
    "epochs = 5  # количество итераций для изменения весов и смещения в процессе обучения\n",
    "h = 0.1  # размер шага изменения весов и смещения\n",
    "\n",
    "# Веса и смещение\n",
    "weights = [-0.5, -1.25]  # начальные веса (эти параметры мы будем \"тренировать\"/изменять в процессе обучения)\n",
    "bias = 1.75  # начальное смещение (этот параметр мы будем \"тренировать\"/изменять в процессе обучения)\n",
    "\n",
    "# Функция активации: ступенчатая/пороговая функция (порог 0.5)\n",
    "def activation(z):\n",
    "    return 1 if z >= 0.5 else 0\n",
    "\n",
    "# Функция предсказания\n",
    "def predict(x):\n",
    "    # z = x1*w1 + x2*w2 + ... + xn*wn + bias\n",
    "    z = sum(xi * wi for xi, wi in zip(x, weights)) + bias  # взвешенная сумма\n",
    "    a = activation(z)  # активация\n",
    "    return a  # возвращаем значение активации (prediction)\n",
    "\n",
    "# Функция обновления/изменения весов и смещения на основе ошибки\n",
    "def update(weights, bias, error, x):\n",
    "    for i in range(len(weights)):\n",
    "        weights[i] += h * error * x[i]\n",
    "    bias += h * error\n",
    "    return weights, bias\n",
    "\n",
    "# Выход/прогноз нашей программы до обучения \n",
    "prediction = predict(x)\n",
    "print(f'Входные данные (x): {x}')\n",
    "print(f'Ожидаемое/необходимое значение (y): {y}')\n",
    "print(f'Начальные значения (до обучения) весов (weights): {weights} и смещения (bias): {bias:.2f}')\n",
    "\n",
    "print('\\nТест до обучения')\n",
    "print(f'Для входных данных {x} предсказанное значение: {prediction}, ожидаемое/истинное значение: {y}')\n",
    "\n",
    "print(f'\\nПроцесс обучения из {epochs} эпох')\n",
    "# Обучение (процесс подбора/изменения весов и смещения для нахождения решения задачи)\n",
    "for epoch in range(epochs):\n",
    "    # Получение предсказания\n",
    "    prediction = predict(x)\n",
    "    \n",
    "    # Вычисление ошибки\n",
    "    error = y - prediction\n",
    "    \n",
    "    # Обновление/изменение весов и смещения на основе ошибки\n",
    "    weights, bias = update(weights, bias, error, x)\n",
    "    \n",
    "    # Отладочная информация\n",
    "    rounded_weights = [round(weight, 2) for weight in weights]  # округляем веса до двух знаков после запятой для удобочитаемости\n",
    "    rounded_bias = round(bias, 2)  # округляем смещение до двух знаков после запятой для удобочитаемости\n",
    "    print(f'Эпоха обучения {epoch + 1}/{epochs}, weights: {rounded_weights}, bias: {rounded_bias}')\n",
    "\n",
    "# Предсказание на обучающем примере\n",
    "print('\\nТест после обучения')\n",
    "prediction = predict(x)\n",
    "print(f'Для входных данных {x} предсказанное значение: {prediction}, ожидаемое/истинное значение: {y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В целом нет смысла разбирать все повторно. Разбор такой же как и предыдущий. Главное понять, что объекты `x` подаваемые в сеть будут иметь свои признаки и сеть должна обучить веса таким образом чтобы правильно классифицировать эти объекты (в случае задачи классификации). На что можно обратить внимание, так это на то, что как видно из результатов, у нас второй вес не обучается. Это происходит потому что наш единственный объект `x` с двумя признаками и один признак равен 0. Поэтому при расчете того, на сколько должен измениться наш второй вес у нас присходит умножение на 0 и следовательно второй вес изменяетс на 0, то есть не изменяется. Следующий шаг это пререход к ООП и представление всей модели сети как совокупность работающих между собой объектов (нейронов/слоев/оптимизаторов и др.) Но перед переходом в ООП сделаем еще один небольшой шажок, добавив в наш набор еще один признак и посмотрим на процесс обучения, чтобы окончательно закрепить понимание того что происходит при обучении."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Почти Логическое И (AND)\n",
    "\n",
    "Что такое Логическое И (AND) и как эту задачу решает наш Перцептрон (нейрон с пороговой функциех активации) мы подробно рассмотрим дальше. А перед окончательным переходом к ООП рассмотрим коротко еще на один пример. Перед тем как написать класс для Перцептрона просто рассмотрим выборку из двух объектов и посмотрим на процесс обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Входные данные (X): [[1, 0], [1, 1]]\n",
      "Ожидаемые/необходимые значения (y): [0, 1]\n",
      "Начальные значения (до обучения) весов (weights): [-0.5, -1.25] и смещения (bias): 1.75\n",
      "\n",
      "Тест до обучения\n",
      "Для входных данных [1, 0] предсказанное значение: 1, ожидаемое/истинное значение: 0\n",
      "Для входных данных [1, 1] предсказанное значение: 0, ожидаемое/истинное значение: 1\n",
      "\n",
      "Процесс обучения из 5 эпох\n",
      "Эпоха обучения 1/5, weights: [-0.5, -0.25], bias: 1.75\n",
      "Эпоха обучения 2/5, weights: [-0.5, 0.75], bias: 1.75\n",
      "Эпоха обучения 3/5, weights: [-0.5, 1.75], bias: 1.75\n",
      "Эпоха обучения 4/5, weights: [-1.5, 1.75], bias: 0.75\n",
      "Эпоха обучения 5/5, weights: [-1.5, 1.75], bias: 0.75\n",
      "\n",
      "Тест после обучения\n",
      "Для входных данных [1, 0] предсказанное значение: 0, ожидаемое/истинное значение: 0\n",
      "Для входных данных [1, 1] предсказанное значение: 1, ожидаемое/истинное значение: 1\n"
     ]
    }
   ],
   "source": [
    "# Общий алгоритм\n",
    "\n",
    "# Входные данные и целевая метка (ожидаемый результат)\n",
    "X = [[1, 0], [1, 1]]  # выборка объектов (два объекта в выборке каждый с двумя признаками)\n",
    "y = [0, 1]  # объект x = [1, 0] класса 0, объект x = [1, 1] класса 1\n",
    "\n",
    "# Гиперпараметры (этими параметрами мы можем влиять на процесс обучения)\n",
    "epochs = 5  # количество итераций для изменения весов и смещения в процессе обучения\n",
    "h = 1  # размер шага изменения весов и смещения\n",
    "\n",
    "# Веса и смещение\n",
    "weights = [-0.5, -1.25]  # начальные веса (эти параметры мы будем \"тренировать\"/изменять в процессе обучения)\n",
    "bias = 1.75  # начальное смещение (этот параметр мы будем \"тренировать\"/изменять в процессе обучения)\n",
    "\n",
    "# Функция активации: ступенчатая/пороговая функция (порог 0.5)\n",
    "def activation(z):\n",
    "    return 1 if z >= 0.5 else 0\n",
    "\n",
    "# Функция предсказания\n",
    "def predict(x):\n",
    "    # z = x1*w1 + x2*w2 + ... + xn*wn + bias\n",
    "    z = sum(xi * wi for xi, wi in zip(x, weights)) + bias  # взвешенная сумма\n",
    "    a = activation(z)  # активация\n",
    "    return a  # возвращаем значение активации (prediction)\n",
    "\n",
    "# Функция обновления/изменения весов и смещения на основе ошибки\n",
    "def update(weights, bias, error, x):\n",
    "    for i in range(len(weights)):\n",
    "        weights[i] += h * error * x[i]\n",
    "    bias += h * error\n",
    "    return weights, bias\n",
    "\n",
    "print(f'Входные данные (X): {X}')\n",
    "print(f'Ожидаемые/необходимые значения (y): {y}')\n",
    "print(f'Начальные значения (до обучения) весов (weights): {weights} и смещения (bias): {bias:.2f}')\n",
    "\n",
    "# Предсказания до обучения\n",
    "print('\\nТест до обучения')\n",
    "for x, y_true in zip(X, y):\n",
    "    prediction = predict(x)\n",
    "    print(f'Для входных данных {x} предсказанное значение: {prediction}, ожидаемое/истинное значение: {y_true}')\n",
    "\n",
    "print(f'\\nПроцесс обучения из {epochs} эпох')\n",
    "# Обучение (процесс подбора/изменения весов и смещения для нахождения решения задачи)\n",
    "for epoch in range(epochs):\n",
    "    for x, y_true in zip(X, y):    \n",
    "        # Получение предсказания\n",
    "        prediction = predict(x)\n",
    "        \n",
    "        # Вычисление ошибки\n",
    "        error = y_true - prediction\n",
    "        \n",
    "        # Обновление/изменение веса и смещения на основе ошибки\n",
    "        weights, bias = update(weights, bias, error, x)\n",
    "    \n",
    "    # Отладочная информация\n",
    "    rounded_weights = [round(weight, 2) for weight in weights]  # округляем веса до двух знаков после запятой для удобочитаемости\n",
    "    rounded_bias = round(bias, 2)  # округляем смещение до двух знаков после запятой для удобочитаемости\n",
    "    print(f'Эпоха обучения {epoch + 1}/{epochs}, weights: {rounded_weights}, bias: {rounded_bias}')\n",
    "\n",
    "# Предсказание на обучающем примере\n",
    "print('\\nТест после обучения')\n",
    "for x, y_true in zip(X, y):\n",
    "    prediction = predict(x)\n",
    "    print(f'Для входных данных {x} предсказанное значение: {prediction}, ожидаемое/истинное значение: {y_true}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В отличии от предыдущего примера, можно заметить, что в процессе обучения обучаются уже оба веса и смещение. Таким образом наша программа с изначальными весами неправильно классифицировала оба объекта, но после 5 эпох обучения веса были подстроены так, что оба объекта стали классифицироваться правильно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Почти Перцептрон"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почему почти станет понятно чуть дальше. Пока давайте разберем код для класса Perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Класс для Перцептрона\n",
    "class Perceptron:\n",
    "    # Встроенный метод для инициализации объекта\n",
    "    def __init__(self, weights, bias=None):\n",
    "        \"\"\"\n",
    "        Инициализирует перцептрон с заданными весами и смещением.\n",
    "        \n",
    "        :param weights: Список начальных весов.\n",
    "        :param bias: Начальное значение смещения. По умолчанию None.\n",
    "        \"\"\"\n",
    "        self.inputs = None                  # входные значения (устанавливаются позже в методе forward)\n",
    "        self.weights = weights              # веса\n",
    "        self.bias = bias                    # смещение\n",
    "        self.z = None                       # взвешенная сумма входов + смещение (вычисляется позже в методе forward)\n",
    "        self.activation = self.heaviside    # функция активации\n",
    "        self.a = None                       # результат применения активационной функции (вычисляется позже в методе forward)\n",
    "    \n",
    "    # Метод для активации (пороговая функция активации)\n",
    "    def heaviside(self, z):\n",
    "        \"\"\"\n",
    "        Пороговая функция активации (функция Хевисайда).\n",
    "        \n",
    "        :param z: Взвешенная сумма входов и смещения.\n",
    "        :return: 1, если z >= 0.5, иначе 0.\n",
    "        \"\"\"\n",
    "        return 1 if z >= 0.5 else 0\n",
    "\n",
    "    # Метод для подачи/\"прогона\" данных в/через перцептрон, вычисления z и активации (выхода/ответа)\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Подает/\"прогоняет\" данные в/через перцептрон, вычисляет взвешенную сумму и применяет функцию активации.\n",
    "        \n",
    "        :param inputs: Список входных значений (набор признаков объекта).\n",
    "        :return: Результат применения функции активации.\n",
    "        \"\"\"\n",
    "        # Сохраняем поданные в перцептрон входные данные в переменной-атрибуте объекта\n",
    "        self.inputs = inputs\n",
    "        # Рассчитываем взвешенную сумму (z)\n",
    "        self.z = sum(xi * wi for xi, wi in zip(self.inputs, self.weights)) + (self.bias if self.bias is not None else 0)\n",
    "        # Применяем функцию активации к z\n",
    "        self.a = self.activation(self.z)\n",
    "        return self.a  # возвращаем результат активации (выход/ответ перцептрона output)\n",
    "    \n",
    "    # Метод для получения предсказаний по всей выборке\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Подает/\"прогоняет\" весь набор данных в/через перцептрон.\n",
    "        \n",
    "        :param X: Список списков или массив входных значений (набор признаков объекта).\n",
    "        :return: Список результатов активации для каждого набора входных значений (для каждого объекта из выборки).\n",
    "        \"\"\"\n",
    "        return [self.forward(x) for x in X]\n",
    "    \n",
    "    # Встроенный метод для строкового представления объекта\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Возвращает строковое представление объекта перцептрон.\n",
    "        \n",
    "        :return: Строка с текущими значениями атрибутов перцептрона (значениями атрибутов рассчитанные при последнем \"прогоне\").\n",
    "        \"\"\"\n",
    "        # self.forward(self.inputs) if self.inputs is not None else ...  # рассчитываем актуальные значения z и a\n",
    "        return f'Perceptron, inputs: {self.inputs}, weights: {self.weights}, bias: {self.bias}, z: {self.z}, ' \\\n",
    "               f'activation: {self.activation.__name__}, a (output): {self.a}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед нами класс с названием `Perceptron`. Для чего он нужен? Нужен он для создания разных объектов этого класса. Что такое объект? Тут приходит на ум только бессмысленное объяснение, вроде того, что объект это объект. Так как это не курс по ООП, я попытаюсь объяснить только общие моменты или только то, что мы непосредственно используем.\n",
    "\n",
    "Итак, объект — это некая сущность ~~в виде гномика~~, которая обладает какими-то свойствами/атрибутами, если мы говорим о классе объекта и какими-то методами (для понимания это некие функции). Этот самый объект может принимать какие-то данные, как та же функция, например, и производить с этими данными какие-то вычисления, менять как-то свое состояние (значение своих параметров), взаимодействовать с другими объектами или функциями при этом в одной программе может быть много объектов разных классов и они могут каким-то образом друг с другом взаимодействовать описывая какие-то сложные процессы и понятия.\n",
    "\n",
    "Наверное, звучит довольно запутанно, но на самом деле, если в этом разобраться, то потом все подряд начинаешь писать в ОО-стиле программирования. Зачем это нужно? Дело в том, что при написании достаточно сложных программ и при решении сложных задач довольно быстро сталкиваешься с тем, что код разрастается, и поддерживать и прослеживать логику становится все сложнее. В коде могут быть сотни и тысячи разных функций и еще больше переменных и параметров. Это все бесконечно усложняет.\n",
    "\n",
    "Поэтому переход к объектно-ориентированной концепции программирования в значительной мере упрощает понимание кода и позволяет описывать более сложные сущности и оперировать более сложными абстракциями, что в свою очередь помогает создавать более сложные и продвинутые программы.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В нашей первоначальной программе были некоторые переменные, которые по смыслу можно отнести к некоторой общей группе или классу. Не путать с понятием класса, которым мы оперируем при классификации наших данных. Имеется в виду, что, например, переменные `weights`, `bias`, `z`, `a` (output) между собой связаны, и их можно представить как некие свойства некоторого объекта. То есть наш объект `perceptron` будет сам в себе хранить веса и смещение, например.\n",
    "\n",
    "Так же и с функциями. Например, функция `forward`, которая принимает некоторые данные и вычисляет `z` и `a`, или функция `predict`, которая принимает нашу выборку и передает по отдельности каждый элемент выборки в `forward`, или функцию активации тоже можно отнести к нашему объекту `perceptron`. То есть эти переменные и эти функции как бы связаны между собой и объединены по смыслу с нашим объектом `perceptron`.\n",
    "\n",
    "Поэтому мы создаем общий класс с названием `Perceptron` и \"помещаем\" в него наши переменные, которые мы называем атрибутами, и функции, которые мы называем методами. Когда мы создаем сам объект `perceptron` на основе этого класса, этот объект хранит в себе свои значения переменных и может быть в разные моменты вызван для произведения нужных нам вычислений. При этом этих объектов может быть и несколько, что нам обязательно нужно будет, когда мы столкнемся с тем, что наш один объект класса перцептрон не способен решить некоторые задачи, а вот уже два этих объекта, работая совместно, эти задачи решают."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Описание атрибутов и методов класса Perceptron:\n",
    "\n",
    "`__init__`\n",
    "Инициализирует перцептрон с заданными весами и смещением.\n",
    "weights - начальные веса.\n",
    "bias - начальное значение смещения (по умолчанию None).\n",
    "heaviside(self, z):\n",
    "\n",
    "`heaviside`\n",
    "Функция активации (функция Хевисайда).\n",
    "Возвращает 1, если z >= 0.5, иначе 0.\n",
    "\n",
    "`forward`\n",
    "Функция подает/\"прогоняет\" данные в/через перцептрон, вычисляет взвешенную сумму и применяет функцию активации.\n",
    "Возвращает результат применения функции активации.\n",
    "\n",
    "`predict`\n",
    "Функция предсказывает выходные значения всего набору данных.\n",
    "Возвращает список результатов активации для каждого набора входных значений.\n",
    "\n",
    "`__str__`\n",
    "Возвращает строковое представление объекта перцептрона с текущими значениями атрибутов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дополнитеьное пояснение методов. У объекта могут быть втроенные или как их еще называют магические методы. В Python они выделяются двумя подчеркиваниями в начале и в конце имени. Например `__new__`, `__init__`, `__call__`, `__srt__` и другие. Эти методы позволяют получать разное поведение объекта в разных условиях. Что это значит? Когда мы создаем объект и пишем строке `perceptron = Perceptron(weights=[-0.5, -1.25], bias=1.75)` то вызывается метод `__init__` нашего класса Perceptron и ему могут быть переданы два параметра: `weights` и `bias`. Причем `weights` обязательно должен быть указан а `bias` если не укажем то примет значение по-умолчанию `None` (на самом деле перед `__init__` будет вызван `__new__` но мы его не используем). То есть это поведение объекта при создании. Если мы например определили метод `__str__`, то при попытке напечатать наш объект: `print(perceptron)` - вызовется метод `__str__` и мы получим строку которую этот метод возвращает. Методы написанные нами вызываются у объекта путем указания их имени через точку. Например в результате выполнения строки `predictions = perceptron.predict(X)` в переменную `prediction` будет записан результат который вернет метод `predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron, inputs: None, weights: [-0.5, -1.25], bias: 1.75, z: None, activation: heaviside, a (output): None\n"
     ]
    }
   ],
   "source": [
    "# Создаем объект класса Perceptron\n",
    "perceptron = Perceptron(weights=[-0.5, -1.25], bias=1.75)\n",
    "# Выводм текстовое представление объекта\n",
    "print(perceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron, inputs: None, weights: [-0.5, -1.25], bias: 1.75, z: None, activation: heaviside, a (output): None\n",
      "\n",
      "Тест до обучения\n",
      "Для входных данных [[1, 0], [1, 1]] предсказанные значения: [1, 0], ожидаемые/истинные значения: [0, 1]\n",
      "\n",
      "Данные \"прогона\" последнего объекта\n",
      "Perceptron, inputs: [1, 1], weights: [-0.5, -1.25], bias: 1.75, z: 0.0, activation: heaviside, a (output): 0\n"
     ]
    }
   ],
   "source": [
    "# Входные данные и целевая метка (ожидаемый результат)\n",
    "X = [[1, 0], [1, 1]]  # выборка объектов (два объекта в выборке каждый с двумя признаками)\n",
    "y = [0, 1]  # объект x = [1, 0] класса 0, объект x = [1, 1] класса 1\n",
    "\n",
    "# Объявляем и инициализируем объект класса Perceptron\n",
    "perceptron = Perceptron(weights=[-0.5, -1.25], bias=1.75)\n",
    "print(perceptron)\n",
    "\n",
    "# Предсказания до обучения\n",
    "print('\\nТест до обучения')\n",
    "predictions = perceptron.predict(X)\n",
    "print(f'Для входных данных {X} предсказанные значения: {predictions}, ожидаемые/истинные значения: {y}')\n",
    "\n",
    "print('\\nДанные \"прогона\" последнего объекта')\n",
    "print(perceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте сделаем еще одну мелочь, а именно добавим в наш класс магический метод `__call__`. Особой необходимости в этом нет, но как говорят: для закрепления пройденого материала. После добавления этого магического метода мы сможем подвать данные в наш объект и получать предсказание просто написав такой код: `predictions = perceprton(X)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Класс для Перцептрона\n",
    "class Perceptron:\n",
    "    # Встроенный метод для инициализации объекта\n",
    "    def __init__(self, weights, bias=None):\n",
    "        self.inputs = None                  # входные значения (устанавливаются позже в методе forward)\n",
    "        self.weights = weights              # веса\n",
    "        self.bias = bias                    # смещение\n",
    "        self.z = None                       # взвешенная сумма входов + смещение (вычисляется позже в методе forward)\n",
    "        self.activation = self.heaviside    # функция активации\n",
    "        self.a = None                       # результат применения активационной функции (вычисляется позже в методе forward)\n",
    "    \n",
    "    # Метод для активации (пороговая функция активации)\n",
    "    def heaviside(self, z):\n",
    "        return 1 if z >= 0.5 else 0\n",
    "\n",
    "    # Метод для подачи/\"прогона\" данных в/через перцептрон, вычисления z и активации (выхода/ответа)\n",
    "    def forward(self, inputs):\n",
    "        # Сохраняем поданные в перцептрон входные данные в переменной-атрибуте объекта\n",
    "        self.inputs = inputs\n",
    "        # Рассчитываем взвешенную сумму (z)\n",
    "        self.z = sum(xi * wi for xi, wi in zip(self.inputs, self.weights)) + (self.bias if self.bias is not None else 0)\n",
    "        # Применяем функцию активации к z\n",
    "        self.a = self.activation(self.z)\n",
    "        return self.a  # возвращаем результат активации (выход/ответ перцептрона output)\n",
    "    \n",
    "    # Встроенный метод для вызова объекта как функции\n",
    "    def __call__(self, X):\n",
    "        return [self.forward(x) for x in X]\n",
    "\n",
    "    # Метод для получения предсказаний по всей выборке\n",
    "    def predict(self, X):\n",
    "        return self.__call__(X)\n",
    "    \n",
    "    # Встроенный метод для строкового представления объекта\n",
    "    def __str__(self):\n",
    "        # self.forward(self.inputs) if self.inputs is not None else ...  # рассчитываем актуальные значения z и a\n",
    "        return f'Perceptron, inputs: {self.inputs}, weights: {self.weights}, bias: {self.bias}, z: {self.z}, ' \\\n",
    "               f'activation: {self.activation.__name__}, a (output): {self.a}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron, inputs: None, weights: [-0.5, -1.25], bias: 1.75, z: None, activation: heaviside, a (output): None\n",
      "\n",
      "Тест до обучения\n",
      "Для входных данных [[1, 0], [1, 1]] предсказанные значения: [1, 0], ожидаемые/истинные значения: [0, 1]\n",
      "\n",
      "Данные \"прогона\" последнего объекта\n",
      "Perceptron, inputs: [1, 1], weights: [-0.5, -1.25], bias: 1.75, z: 0.0, activation: heaviside, a (output): 0\n"
     ]
    }
   ],
   "source": [
    "# Входные данные и целевая метка (ожидаемый результат)\n",
    "X = [[1, 0], [1, 1]]  # выборка объектов (два объекта в выборке каждый с двумя признаками)\n",
    "y = [0, 1]  # объект x = [1, 0] класса 0, объект x = [1, 1] класса 1\n",
    "\n",
    "# Объявляем и инициализируем объект класса Perceptron\n",
    "perceptron = Perceptron(weights=[-0.5, -1.25], bias=1.75)\n",
    "print(perceptron)\n",
    "\n",
    "# Предсказания до обучения\n",
    "print('\\nТест до обучения')\n",
    "predictions = perceptron(X)\n",
    "print(f'Для входных данных {X} предсказанные значения: {predictions}, ожидаемые/истинные значения: {y}')\n",
    "\n",
    "print('\\nДанные \"прогона\" последнего объекта')\n",
    "print(perceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы оперируем таким понятием как перцептрон и можем также оперировать таким понятием как обучение перцептрона. То есть мы будем в цикле из первоначального кода вызывать наш перцептрон подавая в него данные и получая его предсказания. На основе этих предсказаний мы будем обучать его. Но перед этим добавим еще один метод в наш класс. А именно метод который будет обучать/изменять веса и смещение нашего объекта. Ведь логично что функция которая изменяет параметры самого объекта должна принадлежать этому же объекту. Это конечно не обязательно но вполне логично зачем нам оставлять ее как некую отдельную функцию если эта функция все-равно всегда работает только с этим объектом. Итак перенесем функцию `update` в наш класс и станет методом для обновления весов и смещения.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Класс для Перцептрона\n",
    "class Perceptron:\n",
    "    # Встроенный метод для инициализации объекта\n",
    "    def __init__(self, weights, bias=None):\n",
    "        self.inputs = None                  # входные значения (устанавливаются позже в методе forward)\n",
    "        self.weights = weights              # веса\n",
    "        self.bias = bias                    # смещение\n",
    "        self.z = None                       # взвешенная сумма входов + смещение (вычисляется позже в методе forward)\n",
    "        self.activation = self.heaviside    # функция активации\n",
    "        self.a = None                       # результат применения активационной функции (вычисляется позже в методе forward)\n",
    "    \n",
    "    # Метод для активации (пороговая функция активации)\n",
    "    def heaviside(self, z):\n",
    "        return 1 if z >= 0.5 else 0\n",
    "\n",
    "    # Метод для подачи/\"прогона\" данных в/через перцептрон, вычисления z и активации (выхода/ответа)\n",
    "    def forward(self, inputs):\n",
    "        # Сохраняем поданные в перцептрон входные данные в переменной-атрибуте объекта\n",
    "        self.inputs = inputs\n",
    "        # Рассчитываем взвешенную сумму (z)\n",
    "        self.z = sum(xi * wi for xi, wi in zip(self.inputs, self.weights)) + (self.bias if self.bias is not None else 0)\n",
    "        # Применяем функцию активации к z\n",
    "        self.a = self.activation(self.z)\n",
    "        return self.a  # возвращаем результат активации (выход/ответ перцептрона output)\n",
    "    \n",
    "    # Метод для обновления/изменения весов и смещения на основе ошибки\n",
    "    def update(self, error, h, x):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] += h * error * x[i]\n",
    "        self.bias += h * error\n",
    "    \n",
    "    # Встроенный метод для вызова объекта как функции\n",
    "    def __call__(self, X):\n",
    "        return [self.forward(x) for x in X]\n",
    "\n",
    "    # Метод для получения предсказаний по всей выборке\n",
    "    def predict(self, X):\n",
    "        return self.__call__(X)\n",
    "    \n",
    "    # Встроенный метод для строкового представления объекта\n",
    "    def __str__(self):\n",
    "        # self.forward(self.inputs) if self.inputs is not None else ...  # рассчитываем актуальные значения z и a\n",
    "        return f'Perceptron, inputs: {self.inputs}, weights: {self.weights}, bias: {self.bias}, z: {self.z}, ' \\\n",
    "               f'activation: {self.activation.__name__}, a (output): {self.a}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron, inputs: None, weights: [-0.5, -1.25], bias: 1.75, z: None, activation: heaviside, a (output): None\n",
      "\n",
      "Тест до обучения\n",
      "Для входных данных [[1, 0], [1, 1]] предсказанные значения: [1, 0], ожидаемые/истинные значения: [0, 1]\n",
      "\n",
      "Процесс обучения из 5 эпох\n",
      "Эпоха обучения 1/5, weights: [-0.5, -0.25], bias: 1.75\n",
      "Эпоха обучения 2/5, weights: [-0.5, 0.75], bias: 1.75\n",
      "Эпоха обучения 3/5, weights: [-0.5, 1.75], bias: 1.75\n",
      "Эпоха обучения 4/5, weights: [-1.5, 1.75], bias: 0.75\n",
      "Эпоха обучения 5/5, weights: [-1.5, 1.75], bias: 0.75\n",
      "\n",
      "Тест после обучения\n",
      "Для входных данных [[1, 0], [1, 1]] предсказанные значения: [0, 1], ожидаемые/истинные значения: [0, 1]\n",
      "\n",
      "\"Прогоняем\" по-очереди каждый объект выборки через наш обученный перцептрон\n",
      "Perceptron, inputs: [1, 0], weights: [-1.5, 1.75], bias: 0.75, z: -0.75, activation: heaviside, a (output): 0\n",
      "Perceptron, inputs: [1, 1], weights: [-1.5, 1.75], bias: 0.75, z: 1.0, activation: heaviside, a (output): 1\n"
     ]
    }
   ],
   "source": [
    "# Входные данные и целевая метка (ожидаемый результат)\n",
    "X = [[1, 0], [1, 1]]  # выборка объектов (два объекта в выборке каждый с двумя признаками)\n",
    "y = [0, 1]  # объект x = [1, 0] класса 0, объект x = [1, 1] класса 1\n",
    "\n",
    "# Объявляем и инициализируем объект класса Perceptron\n",
    "perceptron = Perceptron(weights=[-0.5, -1.25], bias=1.75)\n",
    "print(perceptron)\n",
    "\n",
    "# Предсказания до обучения\n",
    "print('\\nТест до обучения')\n",
    "predictions = perceptron(X)\n",
    "print(f'Для входных данных {X} предсказанные значения: {predictions}, ожидаемые/истинные значения: {y}')\n",
    "\n",
    "print(f'\\nПроцесс обучения из {epochs} эпох')\n",
    "# Обучение (процесс подбора/изменения весов и смещения для нахождения решения задачи)\n",
    "for epoch in range(epochs):\n",
    "    for x, y_true in zip(X, y):    \n",
    "        # Прогон каждого отдельного одного объекта из выборки через перцептрон\n",
    "        prediction = perceptron.forward(x)\n",
    "        \n",
    "        # Вычисление ошибки\n",
    "        error = y_true - prediction\n",
    "        \n",
    "        # Обновление/изменение веса и смещения на основе ошибки\n",
    "        perceptron.update(error, h, x)\n",
    "    \n",
    "    # Отладочная информация\n",
    "    rounded_weights = [round(weight, 2) for weight in perceptron.weights]  # округляем веса до двух знаков после запятой для удобочитаемости\n",
    "    rounded_bias = round(perceptron.bias, 2)  # округляем смещение до двух знаков после запятой для удобочитаемости\n",
    "    print(f'Эпоха обучения {epoch + 1}/{epochs}, weights: {rounded_weights}, bias: {rounded_bias}')\n",
    "\n",
    "# Предсказание на обучающем примере\n",
    "print('\\nТест после обучения')\n",
    "predictions = perceptron(X)\n",
    "print(f'Для входных данных {X} предсказанные значения: {predictions}, ожидаемые/истинные значения: {y}')\n",
    "\n",
    "print('\\n\"Прогоняем\" по-очереди каждый объект выборки через наш обученный перцептрон')\n",
    "for x in X:\n",
    "    perceptron.forward(x)\n",
    "    print(perceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возможно пока еще не очень понятно зачем все эти абстракции в виде классов, объектов. Кода меньше не стало, процесс обучения в целом не отличается. Какие преимущества? Просто мы пока не усложняли нашу задачу, поэтому и преимущества перехода к ООП пока могут быть не столь очевидны. Но давайте сделаем еще один довольно важный шаг и перенесем весь алгоритм обучения также в наш класс. Ведь обучение тоже напрямую относится к нашему перцептрону. Именно поэтому я и назвал главу \"Почти Перцептрон\". После реализации процесса обучения в самом классе можно сказать что у нас уже полноценный Перцептрон. Это разделение на почти перцептрон и полноценный перцептрон довольно условное. В целом нет какого-то правила что считать полноценным, а что неполноценным перцептроном. Да и в целом многие понятия довольно абстракты и условны. Я сделал такое разделение просто для постепенного продвижения от простого к более сложному. И в дальнейшем, например, мы вообще уберем метод обучения из класса нейрона на основе которого будем строить нейронную сеть, потому что метод обучения будет принадлежать такому классу как нейронная сеть а не каждому объекту нейрона в этой сети. Да и в целом мы несколько уйдем от такого понятия как нейрон. Такой класс нам не нужен будет в том виде в котором мы его реализуем сейчас. Потому что все вычисления будут происходить на уровне таких понятий как \"слой нейронной сети\". Но обо всем по-порядку. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Перцептрон"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перенесем в наш класс метод для обучения перцептрона и решим уже наконец вполне конкретную задачу которая называется \"Логическое И (AND)\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Класс для Перцептрона\n",
    "class Perceptron:\n",
    "    # Встроенный метод для инициализации объекта\n",
    "    def __init__(self, weights, bias=None):\n",
    "        self.inputs = None                  # входные значения (устанавливаются позже в методе forward)\n",
    "        self.weights = weights              # веса\n",
    "        self.bias = bias                    # смещение\n",
    "        self.z = None                       # взвешенная сумма входов + смещение (вычисляется позже в методе forward)\n",
    "        self.activation = self.heaviside    # функция активации\n",
    "        self.a = None                       # результат применения активационной функции (вычисляется позже в методе forward)\n",
    "    \n",
    "    # Метод для активации (пороговая функция активации)\n",
    "    def heaviside(self, z):\n",
    "        return 1 if z >= 0.5 else 0\n",
    "\n",
    "    # Метод для подачи/\"прогона\" данных в/через перцептрон, вычисления z и активации (выхода/ответа)\n",
    "    def forward(self, inputs):\n",
    "        # Сохраняем поданные в перцептрон входные данные в переменной-атрибуте объекта\n",
    "        self.inputs = inputs\n",
    "        # Рассчитываем взвешенную сумму (z)\n",
    "        self.z = sum(xi * wi for xi, wi in zip(self.inputs, self.weights)) + (self.bias if self.bias is not None else 0)\n",
    "        # Применяем функцию активации к z\n",
    "        self.a = self.activation(self.z)\n",
    "        return self.a  # возвращаем результат активации (выход/ответ перцептрона output)\n",
    "    \n",
    "        # Метод для обновления/изменения весов и смещения на основе ошибки\n",
    "    def update(self, error, learning_rate):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] += learning_rate * error * self.inputs[i]\n",
    "        self.bias += learning_rate * error\n",
    "\n",
    "    # Метод для обучения\n",
    "    def fit(self, X, y, epochs, learning_rate):\n",
    "        print(f'\\nПроцесс обучения из {epochs} эпох')\n",
    "        for epoch in range(epochs):\n",
    "            for x, y_true in zip(X, y):    \n",
    "                # Прогон каждого отдельного одного объекта из выборки и получение результата\n",
    "                prediction = self.forward(x)\n",
    "                \n",
    "                # Вычисление ошибки\n",
    "                error = y_true - prediction\n",
    "                \n",
    "                # Обновление/изменение веса и смещения на основе ошибки\n",
    "                self.update(error, learning_rate)\n",
    "            \n",
    "            # Отладочная информация\n",
    "            rounded_weights = [round(weight, 2) for weight in self.weights]  # округляем веса до двух знаков после запятой для удобочитаемости\n",
    "            rounded_bias = round(self.bias, 2)  # округляем смещение до двух знаков после запятой для удобочитаемости\n",
    "            print(f'Эпоха обучения {epoch + 1}/{epochs}, weights: {rounded_weights}, bias: {rounded_bias}')\n",
    "\n",
    "    # Встроенный метод для вызова объекта как функции\n",
    "    def __call__(self, X):\n",
    "        return [self.forward(x) for x in X]\n",
    "\n",
    "    # Метод для получения предсказаний по всей выборке\n",
    "    def predict(self, X):\n",
    "        return self.__call__(X)\n",
    "    \n",
    "    # Встроенный метод для строкового представления объекта\n",
    "    def __str__(self):\n",
    "        # self.forward(self.inputs) if self.inputs is not None else ...  # рассчитываем актуальные значения z и a\n",
    "        return f'Perceptron, inputs: {self.inputs}, weights: {self.weights}, bias: {self.bias}, z: {self.z}, ' \\\n",
    "               f'activation: {self.activation.__name__}, a (output): {self.a}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Логическое И (AND)\n",
    "\n",
    "Что такое \"Логическое И\"? Возможно объяснение, что это такое тут не требуется, так как с этого начинается практический любой нормальный курс по программированию, но все же еще раз вспомним что это такое и дадим пояснения в контексте нейронных сетей.\n",
    "Задача \"Логическое И\" (AND) заключается в создании логической функции, которая возвращает 1 (истина), только если оба входных значения равны 1. В остальных случаях функция возвращает 0 (ложь).\n",
    "\n",
    "Таблица истинности для операции логического И:\n",
    "| Вход A | Вход B | Выход (A AND B) |\n",
    "|--------|--------|-----------------|\n",
    "|   0    |   0    |        0        |\n",
    "|   0    |   1    |        0        |\n",
    "|   1    |   0    |        0        |\n",
    "|   1    |   1    |        1        |\n",
    "\n",
    "В контексте нейронных сетей можно сказать, что, другими словами, мы имеем четыре объекта в нашей выборке. У каждого объекта по два признака (Вход A и Вход B). И каждый объект отнесен к одному из двух классов (Выход). Задача состоит в том чтобы обучить наш перцептрон так, чтобы подавая в него каждый из объектов (каждый набор признаков объекта) наш перцептрон его бы относил к правильному классу. Возможно ли научить этому перцептрон?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron, inputs: None, weights: [-0.5, -1.25], bias: 1.75, z: None, activation: heaviside, a (output): None\n",
      "\n",
      "Тест до обучения\n",
      "Для входных данных [[0, 0], [0, 1], [1, 0], [1, 1]] предсказанные значения: [1, 1, 1, 0], ожидаемые/истинные значения: [0, 0, 0, 1]\n",
      "\n",
      "Процесс обучения из 10 эпох\n",
      "Эпоха обучения 1/10, weights: [0.5, -0.25], bias: 1.75\n",
      "Эпоха обучения 2/10, weights: [1.5, -0.25], bias: 0.75\n",
      "Эпоха обучения 3/10, weights: [1.5, 0.75], bias: -0.25\n",
      "Эпоха обучения 4/10, weights: [2.5, 0.75], bias: -0.25\n",
      "Эпоха обучения 5/10, weights: [2.5, 0.75], bias: -1.25\n",
      "Эпоха обучения 6/10, weights: [2.5, 1.75], bias: -1.25\n",
      "Эпоха обучения 7/10, weights: [2.5, 0.75], bias: -2.25\n",
      "Эпоха обучения 8/10, weights: [2.5, 0.75], bias: -2.25\n",
      "Эпоха обучения 9/10, weights: [2.5, 0.75], bias: -2.25\n",
      "Эпоха обучения 10/10, weights: [2.5, 0.75], bias: -2.25\n",
      "\n",
      "Тест после обучения\n",
      "Для входных данных [[0, 0], [0, 1], [1, 0], [1, 1]] предсказанные значения: [0, 0, 0, 1], ожидаемые/истинные значения: [0, 0, 0, 1]\n",
      "\n",
      "\"Прогоняем\" по-очереди каждый объект выборки через наш обученный перцептрон\n",
      "Perceptron, inputs: [0, 0], weights: [2.5, 0.75], bias: -2.25, z: -2.25, activation: heaviside, a (output): 0\n",
      "Perceptron, inputs: [0, 1], weights: [2.5, 0.75], bias: -2.25, z: -1.5, activation: heaviside, a (output): 0\n",
      "Perceptron, inputs: [1, 0], weights: [2.5, 0.75], bias: -2.25, z: 0.25, activation: heaviside, a (output): 0\n",
      "Perceptron, inputs: [1, 1], weights: [2.5, 0.75], bias: -2.25, z: 1.0, activation: heaviside, a (output): 1\n"
     ]
    }
   ],
   "source": [
    "# Входные данные и целевая метка (ожидаемый результат)\n",
    "X = [[0, 0], [0, 1], [1, 0], [1, 1]]  # выборка объектов (четыре объекта в выборке каждый с двумя признаками)\n",
    "y = [0, 0, 0, 1]  # объекты c признаками [0, 0], [0, 1] и [1, 0] принадлежат классу 0, объект с признаком [1, 1] к классу 1\n",
    "\n",
    "# Объявляем и инициализируем объект класса Perceptron\n",
    "perceptron = Perceptron(weights=[-0.5, -1.25], bias=1.75)\n",
    "print(perceptron)\n",
    "\n",
    "# Предсказания до обучения\n",
    "print('\\nТест до обучения')\n",
    "predictions = perceptron(X)\n",
    "print(f'Для входных данных {X} предсказанные значения: {predictions}, ожидаемые/истинные значения: {y}')\n",
    "\n",
    "# Обучаем перцептрон\n",
    "perceptron.fit(X, y, epochs=10, learning_rate=1)\n",
    "\n",
    "# Предсказание на обучающем примере\n",
    "print('\\nТест после обучения')\n",
    "predictions = perceptron(X)\n",
    "print(f'Для входных данных {X} предсказанные значения: {predictions}, ожидаемые/истинные значения: {y}')\n",
    "\n",
    "print('\\n\"Прогоняем\" по-очереди каждый объект выборки через наш обученный перцептрон')\n",
    "for x in X:\n",
    "    perceptron.forward(x)\n",
    "    print(perceptron)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
